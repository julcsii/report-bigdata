The proposed architecture is robust and flexible for processing large data sets. PySpark is optimal when user defined functions are only used if built-in Spark data frame operations or Pandas user defined functions are not suitable for the problem and performance is negotiable. For a simple Euclidean distance calculation on spatial trajectories, Pandas UDF implementation significantly improves performance in terms of run time compared to standard Spark UDFs. We also found that execution time of data frame operations is lower if Parquet files are used as storage format instead of .csv files.

More conscious partitioning of Parquet files, caching the created data frames and adding larger cluster can boost performance of the Spark application.

