\subsection{Trajectory similarity}
%http://crpit.com/confpapers/CRPITV137Wang.pdf
GPS trajectory mining has been widely researched during the last decades. Since trajectories represent a moving object's location history, there are plenty of use cases where trajectory mining can be successfully applied.  Due the fact that most mobile phones are now able to track GPS, there is more and more data available for this domain of data mining. There is however still no consensus on what dissimilarity or distance measure to use when comparing trajectories.

If we consider trajectories as a sequence of points, the distance of two trajectories can be considered as some sort of aggregate of distances between the points of the compared trajectories. The way of aggregation can determine the computation time, accuracy and sensitivity of the distance measure. 

According to Magdy et al. (\cite{traj-sim-rev}) the existing trajectory distance measures can be classified into two classes: 
\begin{itemize}
    \item \textit{spatial} dissimilarity that aims at finding similarly  shaped trajectories while ignoring the temporal dimension, e.g. Euclidean distance
    \item \textit{spatio-temporal} dissimilarity that considers both the spatial and the temporal dimensions, such as dynamic time-warping (DTW) method
\end{itemize}

In \cite{traj-sim} the authors attempt to provide an experimental study on comparing the six most widely used distance measures. They base the study on a large set of GPS trajectories collected by taxis in Beijing and apply different transformation on top of them. Then they compare the original trajectories - using different dissimilarity measures - to the transformed ones with the assumption that the trajectories with less transformation should be more similar to the originals. One of the dissimilarity measures they used is Euclidean distance, which is a metric measure, also known as L2-norm and according to their results it turned out to be sensitive to noise, sampling rate, but robust to shifts in the trajectory.

Although Euclidean distance relies on the fact that time series and trajectories have similar representations, it does not consider the time-stamps associated with the points in the trajectory. Also the two trajectories need to have the same number of points. Apart from these limitations Euclidean distance is still one of the most commonly used distance measures, since it scales well with large data-sets and is relatively simple. 

On the other hand, DTW is a non-metric measure that provides more flexibility in terms of the length and sampling rate of the trajectories and takes the time-stamps into account. However it is more I/O and time consuming. For low quality GPS data the authors propose longest common sub-sequence (LCSS) method that is also used as a string similarity metric.

Most of the above mentioned measures depend on the definition of spatial distance between two points. One of the most common spatial distance between two points can be given with the Haversine formula \cite{haversine}, which  defines the great-circle distance in kilometers between two points on a sphere given their longitudes and latitudes.

\subsection{Spark}
Apache Spark is an efficient and general-purpose cluster computing system. It provides APIs for Java, Scala, Python and R. Spark runs an optimized engine that supports general execution graphs. Besides the core Spark, there are further high-level tools to support mining of large data sets as illustrated on Figure~\ref{fig:spark}. Spark SQL is built for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for processing data that is continuously generated by different sources.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/spark-stack}
    \caption{Spark stack (Source: \cite{spark})}
    \label{fig:spark}
\end{figure}

Another great advantage of Spark is that it runs with Hadoop, Mesos, standalone, or in the cloud. It can access wide rage of external data sources including Hadoop file system (HDFS), Cassandra, HBase, and S3. 

It also has up to 100 times shorter run-time than Hadoop MapReduce in memory, or 10 times shorter on disk. The reason for this is that Apache Spark has an advanced Directed Acyclic Graph(DAG) execution engine that supports acyclic data flow and in-memory computing.\cite{spark}

To be more efficient, Spark also offers an abstraction called resilient distributed data set (RDD). RDDs can be stored in memory between queries without the need for replication. Instead, the lost data is only rebuilt on failure using lineage graph, meaning that each RDD remembers how it was built and is able to rebuild itself resulting in a more fault-tolerant application. RDDs are one of the main reasons that make it possible for Spark to outperform existing big data processing frameworks. \cite{spark}

Based on \cite{spark:rdds}, RDDs can be created from parallelized collections or from external data sources like Hadoop File System (HDFS), PostgreSQL or S3. They support two types of operations; transformation and actions.

Transformations create new RDDs from existing ones and they are executed only on demand. This means they are executed lazily. However, lazy evaluation has its drawbacks, namely that many partitions\footnote{By default RDDs are partitioned automatically  through different nodes with preset number of partitions. Usually 2-4 partitions per computational unit.} are computed multiple times resulting in longer execution time. A solution to these is RDD persistence with different levels (main memory, local disk or mixed), which means caching RDDs in memory across different operations. 
 
In contrast with transformations, actions launch a computation to return a final value to the program or write data to external storage. Actions triggers execution using lineage graph to load the data into original RDD, carry out all transformations and return  results to the driver program or write it out to file system as shown on Figure \ref{fig:spark-ops}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/operations_spark.png}
    \caption{Operations on RDDs in Spark}
    \label{fig:spark-ops}
\end{figure}

As illustrated on Figure~\ref{fig:spark-cluster} Spark applications run as independent sets of processes on a cluster, navigated by the SparkContext object in the main program, which is called the driver program. The cluster managers allocate resources across applications. There are three types of cluster managers that the SparkContext can connect to:
\begin{enumerate}
\item Apache Mesos, a general cluster manager, 
\item Spark's own built-in cluster manager and
\item YARN, which is the resource manager in Hadoop.
\end{enumerate}
After the connection is established, Spark acquires executors on nodes of the cluster. The executors are processes that run computations and store data for the application. SparkContext sends the application code and tasks to the executors to run.
\cite{spark-cluster}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, height=5cm]{images/cluster-overview}
    \caption{Running Spark on a cluster (Source: \cite{spark-cluster})}
    \label{fig:spark-cluster}
\end{figure}

\subsection{Parquet}
Apache Parquet is a columnar storage format. It is available in the Hadoop ecosystem with every data processing framework, data model or programming language. It is built to support highly efficient compression and encoding schemes. It has been demonstrated that the performance is heavily dependant on applying the right compression and encoding scheme to the data. Compression schemes can be specified on a per-column level in Parquet and allows adding more encoding schemes as they are invented and implemented.
\cite{parquet}

\subsection{Containerization}
Using Linux containers to deploy applications is called containerization. In the recent years it has become increasingly popular thanks to Docker, which is a platform to develop, deploy, and run applications with containers. It is developed to build, secure and manage different applications from development to production both on premises and in the cloud. Developing with Docker containers is flexible, scalable, portable and lightweight.

A container is launched by running an image, which is an executable package that has everything that is required to run the application. The code, a runtime, libraries, environment variables, and configuration files. The container can be referred to as a runtime instance of the respective image. A container runs on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight compared to a virtual machine.\cite{docker}